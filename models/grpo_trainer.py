"""
GRPO (Group Relative Policy Optimization) å®ç°
è®ºæ–‡å¤ç°: https://arxiv.org/abs/2402.03300

æ ¸å¿ƒæ€æƒ³:
1. ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªresponse
2. ä½¿ç”¨reward modelå¯¹responsesæ‰“åˆ†
3. è®¡ç®—groupå†…çš„ç›¸å¯¹ä¼˜åŠ¿(advantage)
4. ç”¨PPO-styleæ›´æ–°ç­–ç•¥
"""

import os
import yaml
import torch
import torch.nn.functional as F
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict
from dataclasses import dataclass
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import PeftModel
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))


@dataclass
class GRPOConfig:
    """GRPOé…ç½®"""
    num_iterations: int = 5
    num_samples_per_prompt: int = 4
    batch_size: int = 2
    learning_rate: float = 5e-6
    kl_coef: float = 0.05
    clip_range: float = 0.2
    gamma: float = 1.0
    reward_scale: float = 1.0
    max_new_tokens: int = 512


class GeminiRewardModel:
    """ä½¿ç”¨Geminiä½œä¸ºreward modelè¯„ä¼°æ•™å­¦è´¨é‡"""
    
    REWARD_PROMPT = """You are evaluating a teaching response for quality and clarity.

Student Question: {question}
Teaching Response: {response}

Rate this response on a scale of 0-10 based on:
1. Clarity and simplicity (Feynman-style)
2. Use of analogies and examples
3. Accuracy of content
4. Engagement and enthusiasm
5. Answering the question directly

Respond with ONLY a number between 0 and 10. No explanation.
"""
    
    def __init__(self, model_name: str = "gemini-2.5-flash"):
        self.model = genai.GenerativeModel(model_name)
    
    def get_reward(self, question: str, response: str) -> float:
        """è·å–å•ä¸ªresponseçš„å¥–åŠ±åˆ†æ•°"""
        try:
            prompt = self.REWARD_PROMPT.format(
                question=question,
                response=response
            )
            result = self.model.generate_content(prompt)
            score = float(result.text.strip())
            return score / 10.0  # å½’ä¸€åŒ–åˆ°[0, 1]
        except Exception as e:
            print(f"Reward error: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    def get_batch_rewards(self, questions: List[str], responses: List[str]) -> List[float]:
        """æ‰¹é‡è·å–å¥–åŠ±"""
        rewards = []
        for q, r in zip(questions, responses):
            rewards.append(self.get_reward(q, r))
        return rewards


class GRPOTrainer:
    """GRPOè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model,
        tokenizer,
        reward_model: GeminiRewardModel,
        config: GRPOConfig,
        ref_model=None
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.reward_model = reward_model
        self.config = config
        self.ref_model = ref_model if ref_model else model
        
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate
        )
    
    def generate_responses(self, prompts: List[str]) -> List[List[str]]:
        """ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªresponses"""
        all_responses = []
        
        for prompt in prompts:
            responses = []
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            for _ in range(self.config.num_samples_per_prompt):
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=self.config.max_new_tokens,
                        do_sample=True,
                        temperature=0.8,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                response = self.tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )
                responses.append(response)
            
            all_responses.append(responses)
        
        return all_responses
    
    def compute_advantages(self, rewards: List[List[float]]) -> List[List[float]]:
        """è®¡ç®—group relative advantages"""
        advantages = []
        
        for group_rewards in rewards:
            # ç»„å†…å½’ä¸€åŒ–
            mean_reward = sum(group_rewards) / len(group_rewards)
            std_reward = (sum((r - mean_reward) ** 2 for r in group_rewards) / len(group_rewards)) ** 0.5
            std_reward = max(std_reward, 1e-8)  # é¿å…é™¤é›¶
            
            group_advantages = [(r - mean_reward) / std_reward for r in group_rewards]
            advantages.append(group_advantages)
        
        return advantages
    
    def compute_kl_divergence(self, log_probs, ref_log_probs):
        """è®¡ç®—KLæ•£åº¦"""
        return (log_probs.exp() * (log_probs - ref_log_probs)).sum(dim=-1).mean()
    
    def train_step(self, prompts: List[str], questions: List[str]):
        """å•æ­¥GRPOè®­ç»ƒ"""
        # 1. ç”Ÿæˆresponses
        all_responses = self.generate_responses(prompts)
        
        # 2. è·å–rewards
        all_rewards = []
        for i, responses in enumerate(all_responses):
            rewards = self.reward_model.get_batch_rewards(
                [questions[i]] * len(responses),
                responses
            )
            all_rewards.append(rewards)
        
        # 3. è®¡ç®—advantages
        advantages = self.compute_advantages(all_rewards)
        
        # 4. ç­–ç•¥æ›´æ–°
        total_loss = 0
        for prompt, responses, advs in zip(prompts, all_responses, advantages):
            for response, advantage in zip(responses, advs):
                # æ„å»ºå®Œæ•´åºåˆ—
                full_text = prompt + response
                inputs = self.tokenizer(full_text, return_tensors="pt").to(self.model.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**inputs, labels=inputs['input_ids'])
                log_probs = F.log_softmax(outputs.logits, dim=-1)
                
                # å‚è€ƒæ¨¡å‹log probs (for KL penalty)
                with torch.no_grad():
                    ref_outputs = self.ref_model(**inputs, labels=inputs['input_ids'])
                    ref_log_probs = F.log_softmax(ref_outputs.logits, dim=-1)
                
                # PPO-style loss
                ratio = (log_probs - ref_log_probs).exp().mean()
                clipped_ratio = torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range)
                
                policy_loss = -torch.min(
                    ratio * advantage,
                    clipped_ratio * advantage
                )
                
                # KL penalty
                kl_loss = self.compute_kl_divergence(log_probs, ref_log_probs)
                
                # æ€»æŸå¤±
                loss = policy_loss + self.config.kl_coef * kl_loss
                total_loss += loss
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'loss': total_loss.item(),
            'avg_reward': sum(sum(r) for r in all_rewards) / sum(len(r) for r in all_rewards),
            'avg_advantage': sum(sum(a) for a in advantages) / sum(len(a) for a in advantages)
        }
    
    def train(self, train_dataset, num_iterations: int = None):
        """æ‰§è¡Œå®Œæ•´GRPOè®­ç»ƒ"""
        if num_iterations is None:
            num_iterations = self.config.num_iterations
        
        print(f"\nğŸš€ Starting GRPO training for {num_iterations} iterations")
        
        for iteration in range(num_iterations):
            print(f"\n{'='*50}")
            print(f"Iteration {iteration + 1}/{num_iterations}")
            print(f"{'='*50}")
            
            # éšæœºé‡‡æ ·batch
            batch_indices = torch.randperm(len(train_dataset))[:self.config.batch_size]
            batch = [train_dataset[int(i)] for i in batch_indices]
            
            prompts = [item['messages'][0]['content'] for item in batch]
            questions = prompts  # å­¦ç”Ÿé—®é¢˜
            
            # è®­ç»ƒæ­¥éª¤
            metrics = self.train_step(prompts, questions)
            
            print(f"ğŸ“Š Loss: {metrics['loss']:.4f}")
            print(f"ğŸ¯ Avg Reward: {metrics['avg_reward']:.4f}")
            print(f"ğŸ“ˆ Avg Advantage: {metrics['avg_advantage']:.4f}")
        
        print("\nâœ… GRPO training complete!")


def load_model_for_grpo(sft_checkpoint_path: str, config: dict):
    """åŠ è½½SFTåçš„æ¨¡å‹ç”¨äºGRPOè®­ç»ƒ"""
    base_model = config['model']['base_model']
    
    # é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        token=os.getenv("HF_TOKEN")
    )
    
    # åŠ è½½LoRAæƒé‡
    model = PeftModel.from_pretrained(model, sft_checkpoint_path)
    
    tokenizer = AutoTokenizer.from_pretrained(
        sft_checkpoint_path,
        trust_remote_code=True
    )
    
    return model, tokenizer


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    # åŠ è½½é…ç½®
    with open("config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½SFTæ¨¡å‹
    sft_path = config['paths']['output_dir'] + "/sft_final"
    print(f"ğŸ“¦ Loading SFT model from {sft_path}")
    model, tokenizer = load_model_for_grpo(sft_path, config)
    
    # åˆ›å»ºreward model
    reward_model = GeminiRewardModel()
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    dataset = load_dataset('json', data_files={'train': 'data/train.jsonl'})
    
    # GRPOé…ç½®
    grpo_cfg = config['grpo']
    grpo_config = GRPOConfig(
        num_iterations=grpo_cfg['num_iterations'],
        num_samples_per_prompt=grpo_cfg['num_samples_per_prompt'],
        batch_size=grpo_cfg['batch_size'],
        learning_rate=grpo_cfg['learning_rate'],
        kl_coef=grpo_cfg['kl_coef'],
        clip_range=grpo_cfg['clip_range'],
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GRPOTrainer(
        model=model,
        tokenizer=tokenizer,
        reward_model=reward_model,
        config=grpo_config
    )
    
    # è®­ç»ƒ
    trainer.train(dataset['train'])
    
    # ä¿å­˜æ¨¡å‹
    output_path = config['paths']['output_dir'] + "/grpo_final"
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    print(f"ğŸ’¾ Model saved to {output_path}")


if __name__ == "__main__":
    main()