"""
ReFeynman - å‘½ä»¤è¡Œæ¼”ç¤º
å¿«é€Ÿæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import argparse


def load_model(checkpoint_path: str):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ“¦ Loading model from {checkpoint_path}...")
    
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B-Instruct",
        load_in_4bit=True,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½LoRAæƒé‡
    model = PeftModel.from_pretrained(base_model, checkpoint_path)
    
    print("âœ… Model loaded!\n")
    return model, tokenizer


def generate_response(model, tokenizer, question: str, max_length: int = 400):
    """ç”Ÿæˆå›ç­”"""
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:],
        skip_special_tokens=True
    )
    
    return response


def interactive_mode(model, tokenizer):
    """äº¤äº’æ¨¡å¼"""
    print("="*60)
    print("ğŸ“ ReFeynman - Interactive Mode")
    print("="*60)
    print("Ask me anything about physics, math, or science!")
    print("Type 'quit' to exit\n")
    
    while True:
        question = input("You: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("\nğŸ‘‹ Goodbye!")
            break
        
        if not question:
            continue
        
        print("\nFeynman: ", end="", flush=True)
        response = generate_response(model, tokenizer, question)
        print(response)
        print("\n" + "-"*60 + "\n")


def main():
    parser = argparse.ArgumentParser(description="ReFeynman CLI Demo")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="checkpoints/grpo_final",
        help="Path to model checkpoint"
    )
    parser.add_argument(
        "--question",
        type=str,
        help="Single question to ask (non-interactive)"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.checkpoint)
    
    if args.question:
        # å•æ¬¡é—®ç­”
        response = generate_response(model, tokenizer, args.question)
        print(f"\nQ: {args.question}")
        print(f"\nA: {response}\n")
    else:
        # äº¤äº’æ¨¡å¼
        interactive_mode(model, tokenizer)


if __name__ == "__main__":
    main()