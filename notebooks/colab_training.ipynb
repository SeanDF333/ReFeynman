{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReFeynman Training Pipeline\n",
    "## 完整的SFT + GRPO训练流程\n",
    "\n",
    "**运行环境**: Google Colab (免费T4 GPU)\n",
    "\n",
    "**训练步骤**:\n",
    "1. 环境配置\n",
    "2. 数据生成 (Gemini API)\n",
    "3. SFT微调 (LoRA)\n",
    "4. GRPO强化学习\n",
    "5. 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆仓库\n",
    "!git clone https://github.com/SeanDF333/ReFeynman.git\n",
    "%cd ReFeynman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置环境变量\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# 在Colab Secrets里添加这些keys\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
    "\n",
    "# 或者直接输入(不安全,仅测试用)\n",
    "# os.environ['HF_TOKEN'] = 'your_token_here'\n",
    "# os.environ['GEMINI_API_KEY'] = 'your_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Gemini生成费曼风格对话数据\n",
    "!python data/generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查生成的数据\n",
    "import json\n",
    "\n",
    "with open('data/train.jsonl', 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "\n",
    "print(\"Sample dialogue:\")\n",
    "print(json.dumps(sample, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: SFT训练 (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行SFT训练\n",
    "# 预计时间: 2-3小时 (T4 GPU)\n",
    "!python models/sft_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: GRPO训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行GRPO强化学习\n",
    "# 预计时间: 1-2小时\n",
    "!python models/grpo_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练好的模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path = \"checkpoints/grpo_final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "\n",
    "print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试推理\n",
    "def ask_feynman(question: str):\n",
    "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=400,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 测试问题\n",
    "questions = [\n",
    "    \"Can you explain quantum entanglement like I'm 10 years old?\",\n",
    "    \"What is calculus and why should I care?\",\n",
    "    \"How do neural networks actually learn?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Feynman: {ask_feynman(q)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 保存到HuggingFace (可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推送到HuggingFace Hub\n",
    "# model.push_to_hub(\"YourUsername/ReFeynman-7B-GRPO\")\n",
    "# tokenizer.push_to_hub(\"YourUsername/ReFeynman-7B-GRPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩checkpoint\n",
    "!zip -r refeynman_checkpoint.zip checkpoints/grpo_final/\n",
    "\n",
    "# 下载\n",
    "from google.colab import files\n",
    "files.download('refeynman_checkpoint.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
